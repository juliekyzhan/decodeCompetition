---
title: "Robustness check - final"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}

output_dir <- "/Users/kunyazhan/Desktop/outputs"
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
}

knitr::opts_chunk$set(
  echo = TRUE,
  fig.path = paste0(output_dir, "/fig-"),  
  dev = "png",      
  dpi = 300,
  fig.width = 7,
  fig.height = 5,
  fig.retina = 2,    
  warning = FALSE,
  message = FALSE
)

```

# 1. Preparation

```{r libraries}
options(repos = c(CRAN = "https://mirrors.tuna.tsinghua.edu.cn/CRAN/"))
install.packages(c(
  "readxl",       
  "readr",       
  "dplyr",        
  "tidyr",       
  "lme4",         
  "MuMIn",       
  "pROC",         
  "knitr",      
  "broom",        
  "kableExtra",   
  "alpaca",       
  "marginaleffects",
  "knitr",
  "brms",
  "tibble",
  "forcats",
  "tidyverse",
  "bayesplot",
  "ggplot2",
  "loo",
  "posterior",
  "cowplot",
  "patchwork",
  "glue",
  "ggrepel"
))


library(readxl)
library(readr)
library(dplyr)
library(tidyr)
library(lme4)
library(MuMIn)
library(pROC)
library(knitr)
library(kableExtra)
library(alpaca)
library(marginaleffects)
library(knitr)
library(tibble)
library(forcats)
library(tidyverse)
library(bayesplot)
library(ggplot2)
library(loo)
library(brms)
library(posterior)
library(cowplot)
library(patchwork)
library(glue)
library(ggrepel)
```

```{r load-data}

adver_path <- file.choose()
legis_path <- file.choose()
adver_data_path <- file.choose()
legis_data_path <- file.choose()

adver_all <- read_excel(adver_path)
legis_all <- read_excel(legis_path)
adver_data <- read_csv(adver_data_path)
legis_data <- read_csv(legis_data_path)

```

# 2.0 train & test data adver

```{r new-split-out-main}

# --- 1. handling low-frequency categories and explicitly encoding NAs ---
lump_low_freq_levels <- function(train_df, test_df, vars, prop = 0.05, other_level = "Other") {
  for (v in vars) {
    if (v %in% names(train_df) && v %in% names(test_df)) {
      train_df[[v]] <- fct_explicit_na(train_df[[v]], na_level = other_level)
      train_df[[v]] <- fct_lump(train_df[[v]], prop = prop, other_level = other_level)
      train_df[[v]] <- droplevels(train_df[[v]])

      allowed_levels <- levels(train_df[[v]])
      test_vec <- as.character(fct_explicit_na(test_df[[v]], na_level = other_level))
      test_vec[!test_vec %in% allowed_levels] <- other_level
      test_df[[v]] <- factor(test_vec, levels = allowed_levels)
    }
  }
  list(train = train_df, test = test_df)
}

# --- 2. factor level alignment ---
align_factor_levels <- function(train, test) {
  factor_vars <- names(train)[sapply(train, is.factor)]
  for (v in factor_vars) {
    if (v %in% names(test)) {
      test[[v]] <- factor(test[[v]], levels = levels(train[[v]]))
    }
  }
  return(test)
}

# --- 3. major cleaning pipeline ---
prepare_data_pipeline <- function(adver_all, adver_data, match_vars, cat_vars_to_lump, numeric_vars) {
  set.seed(123)

  # â‘  rename matching features before merging
  adver_data_raw <- adver_data %>%
    rename_with(.cols = all_of(match_vars), .fn = ~ paste0(.x, "_raw"))

  adver_all <- adver_all %>%
    left_join(adver_data_raw %>% select(full_name, issue, congress, ends_with("_raw")),
              by = c("full_name", "issue", "congress"))

  # â‘ .5 merging the three categories variables into a single main variable
  # to reduced the conlinearity while saving more information
  adver_all <- adver_all %>%
    mutate(
      industry_main = coalesce(industry_tp1, industry_tp2, industry_tp3),
      industry_main = fct_lump(fct_explicit_na(industry_main, na_level = "NA"), n = 10),
      ethnicity_tp_main = coalesce(ethnicity_tp1, ethnicity_tp2, ethnicity_tp3),
      ethnicity_tp_main = fct_lump(fct_explicit_na(ethnicity_tp_main, na_level = "NA"), n = 8),
      employ_main = coalesce(employ1, employ2, employ3),
      payroll_main = coalesce(payroll1, payroll2, payroll3),
      ethnicity_main = coalesce(ethnicity1, ethnicity2, ethnicity3)
    )

  # â‘¡ split the training and test sets
  adver_all$pair_id <- paste(adver_all$full_name, adver_all$issue, sep = "_")
  unique_pairs <- unique(adver_all$pair_id)
  train_units <- sample(unique_pairs, size = floor(0.7 * length(unique_pairs)))

  adver_all <- adver_all %>%
    mutate(set = ifelse(pair_id %in% train_units, "train", "test"))

  train_data <- adver_all %>% filter(set == "train")
  test_data <- adver_all %>% filter(set == "test" & full_name %in% train_data$full_name & issue %in% train_data$issue)

  # â‘¢ factorize all character-type variables along with specified variables
  vars_to_factor <- c(names(train_data)[sapply(train_data, is.character)], "leverage", "ppvi") %>% unique()
  for (v in vars_to_factor) {
    if (v %in% names(train_data)) {
      train_data[[v]] <- factor(train_data[[v]]) |> droplevels()
    }
    if (v %in% names(test_data)) {
      test_data[[v]] <- factor(test_data[[v]], levels = levels(train_data[[v]]))
    }
  }

  # â‘£ applying #1 lumping and explicit NA 
  lumped <- lump_low_freq_levels(train_data, test_data, cat_vars_to_lump)
  train_data <- lumped$train
  test_data <- lumped$test

  # â‘¤ align all factor variables
  test_data <- align_factor_levels(train_data, test_data)

  # â‘¥ median imputation and z-score standardization for numeric variables
  for (v in numeric_vars) {
    if (v %in% names(train_data)) {
      med <- median(train_data[[v]], na.rm = TRUE)
      train_data[[v]][is.na(train_data[[v]])] <- med
      test_data[[v]][is.na(test_data[[v]])] <- med

      mean_v <- mean(train_data[[v]], na.rm = TRUE)
      sd_v <- sd(train_data[[v]], na.rm = TRUE)

      train_data[[v]] <- (train_data[[v]] - mean_v) / sd_v
      test_data[[v]] <- (test_data[[v]] - mean_v) / sd_v
    }
  }

  # â‘¥.5 convert '_raw' features that should be factors into factor
  match_features <- paste0(match_vars, "_raw")
  cat_match_vars <- setdiff(match_features, c("party_score_pl_raw", "party_score_ne_raw"))
  for (v in cat_match_vars) {
    if (v %in% names(train_data)) {
      train_data[[v]] <- factor(train_data[[v]]) |> droplevels()
    }
    if (v %in% names(test_data)) {
      test_data[[v]] <- factor(test_data[[v]], levels = levels(train_data[[v]]))
    }
  }

  # â‘¦ ensure consistent data types across all specified variables
  all_vars <- unique(c(
    match_features,
    "leverage", "s_committee_tp1", "h_committee_tp1", "party_tp_type", "party_tp_score",
    "industry_main", "ethnicity_main", "employ_main", "payroll_main",
    "ppvi", "npvi",
    "chamber", "gender", "party", "committee_el", "issue", "full_name", "congress"
  ))

  for (v in all_vars) {
    if (v %in% names(train_data) && v %in% names(test_data)) {
      train_class <- class(train_data[[v]])
      if ("factor" %in% train_class) {
        test_data[[v]] <- factor(test_data[[v]], levels = levels(train_data[[v]]))
      } else if ("character" %in% train_class) {
        train_data[[v]] <- factor(train_data[[v]]) |> droplevels()
        test_data[[v]] <- factor(test_data[[v]], levels = levels(train_data[[v]]))
      } else if ("numeric" %in% train_class || "integer" %in% train_class) {
        test_data[[v]] <- as.numeric(test_data[[v]])
      }
    }
  }

  list(train = train_data, test = test_data)
}

# call 

match_vars <- c("leverage_pl", "leverage_ne", "committee_pl", "committee_ne",
                "employ_industry_pl", "employ_industry_ne",
                "payroll_industry_pl", "payroll_industry_ne",
                "ethnicity_pl", "ethnicity_ne",
                "party_value_pl", "party_value_ne",
                "party_score_pl", "party_score_ne")

cat_vars_to_lump <- c(
  "h_committee_tp1", "s_committee_tp1", 
  "industry_tp1", "industry_tp2", "industry_tp3", 
  "ethnicity_tp1", "ethnicity_tp2", "ethnicity_tp3",
  "employ1", "employ2", "employ3", 
  "payroll1", "payroll2", "payroll3",
  "ethnicity1", "ethnicity2", "ethnicity3"
)

numeric_vars <- c("party_tp_score", "npvi")

processed <- prepare_data_pipeline(
  adver_all, 
  adver_data, 
  match_vars, 
  cat_vars_to_lump, 
  numeric_vars
)

train_data_adver <- processed$train
test_data_adver <- processed$test
```

## 2.1. Predictive Validity adver

### 2.1.1 models setting adver

```{r bayes-model-adver}

# four formulas

formula_base <- bf(label ~ 1 + (1|issue) + (1|full_name) + (1|congress))


# introducing baseline model: 
# baseline model uses standard legislator-lecel covariates commonly used in political science
# to evaluate the explanatory power of our mechanism-based features
# here we focused on the bayes-r2 of model baseline and raw, match

formula_baseline <- bf(label ~ chamber + age + gender + party + seniority +
                         ideology + (1|issue) + (1|full_name) + (1|congress))

formula_raw <- bf(label ~ leverage + s_committee_tp1 + h_committee_tp1 +
                    party_tp_type + party_tp_score + industry_main + ethnicity_tp_main
                  + employ_main + payroll_main +ethnicity_main + ppvi + npvi +
                          (1|issue) + (1|full_name) + (1|congress))

match_features <- paste0(match_vars, "_raw")
formula_match <- bf(
  as.formula(paste("label ~", paste(match_features, collapse = " + "), "+ (1|issue) + (1|full_name) + (1|congress)"))
)


# check parameter
priors_base_check <- get_prior(formula_base, data = train_data_adver, family = bernoulli())
priors_baseline_check <- get_prior(formula_baseline, data = train_data_adver, family = bernoulli())
priors_raw_check  <- get_prior(formula_raw, data = train_data_adver, family = bernoulli())
priors_match_check <- get_prior(formula_match, data = train_data_adver, family = bernoulli())

# set priors for models
# - normal prior (mean=0, sd=2.5) for all fixed effect regression coefficients (class = "b")
# reflecting the expection that most coefficients are centered around zero with moderate variability.
# - heavy-tailed student-t prior (df=3, mean=0, scale=2.5) for the intercept (class = "Intercept")
# and group-levels standard deviations (class = "sd")
# which improves model robustness to outliers and enhances interpretability by allowing more flexibility
# in the presence of extreme values

priors_base <- c(
  set_prior("student_t(3, 0, 2.5)", class = "Intercept"),
  set_prior("student_t(3, 0, 2.5)", class = "sd")
)

priors_mechanism <- c(
  set_prior("normal(0, 2.5)", class = "b"),         
  set_prior("student_t(3, 0, 2.5)", class = "Intercept"),
  set_prior("student_t(3, 0, 2.5)", class = "sd")
)

# iteration parameters setting
# n_iter = 1000 balances between sufficient exploration of the parameter space
# and the computational efficienct
# n_chains = 2 helps assess convergence and ensures robustness of the results,
# while keeping computational cost manageable.

n_iter <- 1000
n_chains <- 2

# model training
bayes_base <- brm(formula_base, data = train_data_adver, family = bernoulli(),
                  prior = priors_base, iter = n_iter, chains = n_chains,
                  seed = 123, control = list(adapt_delta = 0.9))

bayes_baseline <- brm(formula_baseline, data = train_data_adver, family = bernoulli(),
                 prior = priors_mechanism, iter = n_iter, chains = n_chains,
                 seed = 123, control = list(adapt_delta = 0.9))

bayes_raw <- brm(formula_raw, data = train_data_adver, family = bernoulli(),
                 prior = priors_mechanism, iter = n_iter, chains = n_chains,
                 seed = 123, control = list(adapt_delta = 0.9))

bayes_match <- brm(formula_match, data = train_data_adver, family = bernoulli(),
                   prior = priors_mechanism, iter = n_iter, chains = n_chains,
                   seed = 123, control = list(adapt_delta = 0.9))

# LOO
loo_base <- loo(bayes_base)
loo_baseline <- loo(bayes_baseline)
loo_raw <- loo(bayes_raw)
loo_match <- loo(bayes_match)


# ==== Function: bayes_r2_brms()  ====
# this section defines a function used for calculating bayes r2

bayes_r2_brms <- function(model) bayes_R2(model)[1]

results <- tibble(
  Model = c("Base","Baseline", "Raw", "Match"),
  LOOIC = c(loo_base$estimates["looic", "Estimate"],
            loo_baseline$estimates["looic", "Estimate"],
            loo_raw$estimates["looic", "Estimate"],
            loo_match$estimates["looic", "Estimate"]),
  Bayes_R2 = c(bayes_r2_brms(bayes_base),
                bayes_r2_brms(bayes_baseline),
                bayes_r2_brms(bayes_raw),
                bayes_r2_brms(bayes_match))
)

results %>% kable(caption = "Bayesian Model Comparison: LOOIC and Approximate Pseudo-R2")
```

### 2.1.2 posterior diagnosis adver

```{r posterior-diagnosis}

# ==== Function: bayes_posterior_diagnosis() ====
# this section defines a function to summarize bayesian models diagnostics and visualize posterior results:
# - 1. checks convergence diagnostics (rhat, ess)
# - 2. plots posterior distributions and trace plots
# - 3. visualizes fixed effects estimates with 95% ci
# - 4. optionally saves plots to file

bayes_posterior_diagnosis <- function(model, 
                                                 model_name = "MODEL",
                                                 n_trace = 8,
                                                 loo_list = NULL,
                                                 save_path_prefix = NULL) {

  rhat_vals <- rhat(model)
  ess_vals <- neff_ratio(model)

  cat(glue("------ {model_name} ------\n"))
  cat("Rhat > 1.01:\n")
  print(rhat_vals[rhat_vals > 1.01])

  cat("\nESS < 0.1:\n")
  print(ess_vals[ess_vals < 0.1])

  cat("\nSummary of Rhat:\n")
  print(summary(rhat_vals))
  
  cat("\nSummary of ESS ratio:\n")
  print(summary(ess_vals))

posterior_plot <- mcmc_areas(
    as_draws_df(model),
    regex_pars = "^b_",
    prob = 0.8, prob_outer = 0.95
  ) +
    labs(title = glue("Posterior Distributions with 95% CI ({model_name})"))
  
draw_vars <- variables(as_draws_df(model))
trace_candidates <- draw_vars[grepl("^b_", draw_vars)]

if (length(trace_candidates) == 0) {
  warning(glue("No traceable fixed effect parameters for model {model_name}"))
  trace_plot <- ggplot() + labs(title = glue("Trace Plot - {model_name} (No traceable parameters)"))
} else {
  pars_to_plot <- head(trace_candidates, n_trace)
  trace_plot <- mcmc_trace(as_draws_df(model), pars = pars_to_plot) +
    ggtitle(glue("Trace Plot - {model_name} (First {length(pars_to_plot)} parameters)"))
}

  fixef_df <- fixef(model) %>%
    as.data.frame() %>%
    rownames_to_column("Variable") %>%
    arrange(desc(abs(Estimate)))

  fixef_plot <- ggplot(fixef_df, aes(x = reorder(Variable, Estimate), y = Estimate)) +
    geom_point() +
    geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
    coord_flip() +
    labs(title = glue("Fixed Effects Estimates with 95% CI ({model_name})"),
         y = "Estimate", x = "") +
    theme_minimal()

if (!is.null(save_path_prefix)) {
  dir_path <- dirname(save_path_prefix)
  if (!dir.exists(dir_path)) {
    dir.create(dir_path, recursive = TRUE)
  }

  ggsave(paste0(save_path_prefix, "_posterior.png"), posterior_plot, width = 6, height = 4)
  ggsave(paste0(save_path_prefix, "_trace.png"), trace_plot, width = 8, height = 4)
  ggsave(paste0(save_path_prefix, "_fixef.png"), fixef_plot, width = 6, height = 4)
}

  return(list(
    posterior_plot = posterior_plot,
    trace_plot = trace_plot,
    fixef_plot = fixef_plot
  ))
}

# ==== Use of Function: bayes_posterior_diagnosis() ====

results_base <- bayes_posterior_diagnosis(
  model = bayes_base,
  model_name = "BASE",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_base_diagnosis"
)

results_baseline <- bayes_posterior_diagnosis(
  model = bayes_baseline,
  model_name = "BASELINE",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_baseline_diagnosis"
)

results_raw <- bayes_posterior_diagnosis(
  model = bayes_raw,
  model_name = "RAW",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_raw_diagnosis"
)


results_match <-bayes_posterior_diagnosis(
  model = bayes_match,
  model_name = "MATCH",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_match_diagnosis"
)

```

### 2.1.3 auc comparsion adver

```{r auc-coomparion-adver}

# ==== Function: compare_bayes_auc() ====
# this section defines a function to compare auc performance of multiple bayesian logistic models:
# including:
# - 1. computes predicted propabilities on test data
# - 2. plots roc curves for visual comparison
# - 3. performs pairwaise bootsrap tests of auc differences
# - 4. outputs a summary table of auc values


compare_bayes_auc <- function(models_named_list, 
                              test_data, 
                              label_var = "label", 
                              caption = "AUC Comparison on Test Set",
                              colors = NULL,
                              save_path = NULL) {

  model_names <- names(models_named_list)
  if (is.null(colors)) {
    colors <- RColorBrewer::brewer.pal(min(length(models_named_list), 8), "Set1")
  }

  # 1. predicted probabilities & roc
  roc_list <- list()
  auc_list <- c()

  for (i in seq_along(models_named_list)) {
    model <- models_named_list[[i]]
    name  <- model_names[i]
    
    # posterior mean of expected probabilities
    pred_probs <- posterior_epred(model, newdata = test_data, allow_new_levels = TRUE) %>% colMeans()
    
    test_data[[paste0("pred_", name)]] <- pred_probs
    
    # ROC
    roc_obj <- roc(test_data[[label_var]], pred_probs)
    roc_list[[name]] <- roc_obj
    auc_list[i] <- auc(roc_obj)
  }

  # 2. pic
  plot(roc_list[[1]], col = colors[1], lwd = 2, main = "Bayesian Logistic Models: ROC Comparison")
  for (i in 2:length(roc_list)) {
    lines(roc_list[[i]], col = colors[i], lwd = 2)
  }
  legend("bottomright", legend = model_names, col = colors[1:length(roc_list)], lty = 1, lwd = 2)

  if (!is.null(save_path)) {
    dev.copy(png, filename = save_path, width = 800, height = 600)
    dev.off()
  }

# 3. pairwise comparisons
if (length(roc_list) >= 2) {
  cat("\n--- AUC Bootstrap Tests ---\n")
  for (i in 1:(length(roc_list) - 1)) {
    for (j in (i + 1):length(roc_list)) {
      cat(glue::glue("\n{model_names[i]} vs {model_names[j]}:\n"))
      print(roc.test(roc_list[[i]], roc_list[[j]], method = "bootstrap", paired = TRUE))
    }
  }
}

  # 4. AUC tables
  auc_table <- tibble(
    Model = model_names,
    AUC = auc_list
  )
  
  print(knitr::kable(auc_table, digits = 3, caption = caption))
  
  return(list(
    roc_list = roc_list,
    auc_table = auc_table
  ))
}

# ==== Use of Function: compare_bayes_auc() ====

# models list
models_adver_compare <- list(
  Base = bayes_base,
  Baseline = bayes_baseline,
  Raw = bayes_raw,
  Match = bayes_match
)

# auc comparsion and tables outprint
compare_bayes_auc(models_adver_compare, test_data = test_data_adver)
```

## 2.2. discriminant validility adver

```{r discriminant-bayes-models}

# 1. model setting 

formula_base <- bf(label ~ 1 + (1|issue) + (1|full_name) + (1|congress))


formula_intl <- bf(label ~ leverage_pl_raw + leverage_ne_raw + (1|issue) + (1|full_name) + (1|congress))


formula_intldom <- bf(label ~ leverage_pl_raw + leverage_ne_raw + ethnicity_pl_raw + ethnicity_ne_raw +
                    payroll_industry_pl_raw + payroll_industry_ne_raw +
                    party_value_pl_raw + party_value_ne_raw +
                    party_score_pl_raw + party_score_ne_raw +
                    employ_industry_pl_raw + employ_industry_ne_raw + 
                    (1|issue) + (1|full_name) + (1|congress))

formula_domprof <- bf(label ~ committee_pl_raw + committee_ne_raw +  ethnicity_pl_raw + ethnicity_ne_raw +
                    payroll_industry_pl_raw + payroll_industry_ne_raw +
                    party_value_pl_raw + party_value_ne_raw +
                    party_score_pl_raw + party_score_ne_raw +
                    employ_industry_pl_raw + employ_industry_ne_raw + 
                    (1|issue) + (1|full_name) + (1|congress))

formula_intlprof <- bf(label ~ leverage_pl_raw + leverage_ne_raw + committee_pl_raw + committee_ne_raw + 
                    (1|issue) + (1|full_name) + (1|congress))


formula_match <- bf(
  as.formula(paste("label ~", paste(match_features, collapse = " + "), "+ (1|issue) + (1|full_name) + (1|congress)"))
)


# 2. prior setting 

# keep prior_base
prior_base <- c(
  set_prior("student_t(3, 0, 2.5)", class = "Intercept"),
  set_prior("student_t(3, 0, 2.5)", class = "sd")
)

# change prior_raw/match into prior_mechanism
prior_mechanism <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 2.5)", class = "Intercept"),
  set_prior("student_t(3, 0, 2.5)", class = "sd")
)


# 3. model training

bayes_base <- brm(formula_base, data = train_data_adver, family = bernoulli(),
                  prior = prior_base, iter = 1000, chains = 2, seed = 123)

bayes_intl <- brm(formula_intl, data = train_data_adver, family = bernoulli(),
                  prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_intldom <- brm(formula_intldom, data = train_data_adver, family = bernoulli(),
                 prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_domprof <- brm(formula_domprof, data = train_data_adver, family = bernoulli(),
                  prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_intlprof <- brm(formula_intlprof, data = train_data_adver, family = bernoulli(),
                  prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_match <- brm(formula_match, data = train_data_adver, family = bernoulli(),
                  prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)


# 4.  Bayesian RÂ²

# ==== Use of Function: bayes_r2_brms() ====

r2_mechanism_models_bayes <- tibble::tibble(
  Model = c("BASE", "INTL", "INTLDOM", "DOMPROF", "INTLPROF", "ALL"),
  Pseudo_R2 = c(bayes_r2_brms(bayes_base),
                bayes_r2_brms(bayes_intl),
                bayes_r2_brms(bayes_intldom),
                bayes_r2_brms(bayes_domprof),
                bayes_r2_brms(bayes_intlprof),
                bayes_r2_brms(bayes_match))
)

# 5. tables outprint
knitr::kable(r2_mechanism_models_bayes, caption = "Bayesian Pseudo RÂ² Comparison (Mechanism Models)")

# 6. loo comparison

loo_base <- loo(bayes_base)
loo_intl <- loo(bayes_intl)
loo_intldom  <- loo(bayes_intldom)
loo_domprof <- loo(bayes_domprof)
loo_intlprof <- loo(bayes_intlprof)
loo_match <- loo(bayes_match)

loo_intl$estimates["elpd_loo", "Estimate"]
loo_intldom$estimates["elpd_loo", "Estimate"]
loo_domprof$estimates["elpd_loo", "Estimate"]
loo_intlprof$estimates["elpd_loo", "Estimate"]
loo_match$estimates["elpd_loo", "Estimate"]

# tables compares loo
loo_compare_mechanism <- loo_compare(loo_base, loo_intl, loo_intldom, loo_domprof, loo_intlprof, loo_match)
print(loo_compare_mechanism)

# visualization loo comparison
plot(loo_compare_mechanism, label_points = TRUE, main = "Bayesian Mechanism Models: LOOIC Comparison")
```

### 2.2.1 variables importance comparions adver

```{r posterior-diagnosis-dscriminant-adver, message=FALSE, warning=FALSE}

# set saves path
save_dir <- "outputs/bayes_value_importance"

# ==== Function: bayes_value_importance_v2() ====
# this section defines a function visualize and export posterior fixed effect estimates from bayes model
# - 1. ranks predictions by absolute logit coefficient
# - 2. plots estimates with 95% ci
# - 3. optionally saves the plot and the table to disk

bayes_value_importance_v2 <- function(model, model_name = "Model", 
                                      save_dir = "outputs", 
                                      save_plot = TRUE, save_table = TRUE) {
  if (!dir.exists(save_dir)) dir.create(save_dir, recursive = TRUE)

  effects <- as.data.frame(fixef(model)) %>%
    tibble::rownames_to_column("Variable") %>%
    dplyr::filter(!(Variable %in% c("(Intercept)", "Intercept"))) %>%
    dplyr::arrange(desc(abs(Estimate)))

  p <- ggplot(effects, aes(x = reorder(Variable, abs(Estimate)), y = Estimate)) +
    geom_col(fill = "skyblue") +
    geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
    coord_flip() +
    labs(
      title = paste("Posterior Estimates (with 95% CI) -", model_name),
      x = NULL, y = "Logit Coefficient"
    ) +
    theme_minimal(base_size = 13)

  if (save_plot) {
    plot_path <- file.path(save_dir, paste0("bayes_coef_plot_", model_name, ".png"))
    ggsave(plot_path, p, width = 7, height = 5)
    message(glue::glue("âœ… plot saves to: {plot_path}"))
  }

  if (save_table) {
    table_path <- file.path(save_dir, paste0("bayes_coef_table_", model_name, ".csv"))
    readr::write_csv(effects, table_path)
    message(glue::glue("ðŸ“„ tables saves to: {table_path}"))
  }

  return(list(plot = p, data = effects))
}

# ==== Use of Function: bayes_value_importance_v2() ====

base_result     <- bayes_value_importance_v2(bayes_base,     model_name = "BASE",     save_dir = save_dir)
intl_result     <- bayes_value_importance_v2(bayes_intl,     model_name = "INTL",     save_dir = save_dir)
intldom_result  <- bayes_value_importance_v2(bayes_intldom,  model_name = "INTLDOM",  save_dir = save_dir)
domprof_result  <- bayes_value_importance_v2(bayes_domprof,  model_name = "DOMPROF",  save_dir = save_dir)
intlprof_result <- bayes_value_importance_v2(bayes_intlprof, model_name = "INTLPROF", save_dir = save_dir)
match_result    <- bayes_value_importance_v2(bayes_match,    model_name = "MATCH",    save_dir = save_dir)
```

### 2.2.2 comparions with shap values adver

```{r shap-direction-bayes-adver, message=FALSE, warning=FALSE}

# the following three functions was meant to complete: 
# compares bayesian model coefficients with SHAP values from lgbm models, specifically
# - 1. aligns variables and compares directionality (signs)
# - 2. computes pearson correlation and direction match rate
# - 3. visualizes SHAP vs bayesian coefficients in a scatter plot
# major goal: assessing model interpretability consistenct across frameworks


# ==== Function 1: Extract & Match Coefficients with SHAP ====

compare_bayes_vs_shap_data <- function(
  bayes_model,
  shap_data,
  set_name = "test",
  match_vars = NULL,
  suffix = "_raw"
) {
  # 1. extract fixed effect estimates from bayes model (exclude intercept)
  coef_bayes <- fixef(bayes_model)[, "Estimate"]
  coef_names <- names(coef_bayes)
  coef_names <- coef_names[!(coef_names %in% c("(Intercept)", "Intercept"))]

  # 2. construct match variables names using suffix logic
  if (!is.null(match_vars)) {
    match_vars_with_suffix <- paste0(match_vars, suffix)
    vars_in_model <- intersect(coef_names, match_vars_with_suffix)
  } else {
    vars_in_model <- coef_names
  }

  if (length(vars_in_model) == 0) {
    stop("No matched variables found in Bayesian model. Please check match_vars and suffix.")
  }

  # 3. clean suffix for merging with shap_data
  coef_bayes_tbl <- tibble::tibble(
    var = vars_in_model,
    coef_logit = coef_bayes[vars_in_model],
    var_clean = stringr::str_remove(vars_in_model, paste0(suffix, "$"))
  )

  # 4. extract SHAP values
  vars_shap <- coef_bayes_tbl$var_clean
  missing_vars <- setdiff(vars_shap, colnames(shap_data))
  if (length(missing_vars) > 0) {
    stop(glue::glue("The following variables are missing in shap_data: {paste(missing_vars, collapse = ', ')}"))
  }

  shap_values <- shap_data %>%
    dplyr::filter(set == set_name) %>%
    dplyr::summarise(dplyr::across(all_of(vars_shap), mean, na.rm = TRUE)) %>%
    tidyr::pivot_longer(cols = everything(), names_to = "shap_var", values_to = "shap_value")

  # 5. combine & compute direction/magnitude
  comparison <- coef_bayes_tbl %>%
    dplyr::left_join(shap_values, by = c("var_clean" = "shap_var")) %>%
    dplyr::mutate(
      dir_logit = sign(coef_logit),
      dir_shap = dplyr::if_else(shap_value == 0, NA, sign(shap_value)),
      abs_coef_logit = abs(coef_logit),
      abs_shap_value = abs(shap_value),
      direction_match = (dir_logit == dir_shap),
      direction_mismatch = !direction_match
    )

  # 6. summary stats
  summary_stats <- tibble::tibble(
    Direction_Match_Rate = mean(comparison$direction_match, na.rm = TRUE),
    Pearson_Correlation = cor(comparison$coef_logit, comparison$shap_value, use = "complete.obs"),
    Abs_Value_Correlation = cor(comparison$abs_coef_logit, comparison$abs_shap_value, use = "complete.obs")
  )

  return(list(summary = summary_stats, detail = comparison))
}

# ==== Function 2: Plot SHAP vs Coefficients ====
plot_bayes_vs_shap <- function(comparison_df, model_name = NULL) {
  ggplot2::ggplot(comparison_df, ggplot2::aes(x = coef_logit, y = shap_value, label = var_clean)) +
    ggplot2::geom_point(ggplot2::aes(color = abs_shap_value, shape = direction_mismatch), size = 3) +
    ggplot2::geom_text(vjust = -0.8, size = 3) +
    ggplot2::scale_color_gradient(low = "orange", high = "red", name = "Mean |SHAP|") +
    ggplot2::scale_shape_manual(values = c(`FALSE` = 16, `TRUE` = 17), labels = c("Match", "Mismatch"), name = "Direction Match") +
    ggplot2::geom_vline(xintercept = 0, linetype = "dashed", color = "gray60") +
    ggplot2::geom_hline(yintercept = 0, linetype = "dashed", color = "gray60") +
    ggplot2::labs(
      x = "Posterior Mean (Bayesian logit)",
      y = "Average SHAP Value (LGBM)",
      title = paste("SHAP vs Bayesian Coefficient Directions", model_name)
    ) +
    ggplot2::theme_minimal()
}

# ==== Function 3: Run batch comparisons safely ====
run_bayes_shap_all_models <- function(model_list, shap_data, match_vars, set_name = "test", suffix = "_raw") {
  purrr::imap(model_list, ~{
    tryCatch({
      result <- compare_bayes_vs_shap_data(
        bayes_model = .x,
        shap_data   = shap_data,
        set_name    = set_name,
        match_vars  = match_vars,
        suffix      = suffix
      )
      result$plot <- plot_bayes_vs_shap(result$detail, model_name = .y)
      result
    }, error = function(e) {
      message(glue::glue("âŒ Model {.y} error: {e$message}"))
      return(NULL)
    })
  }) %>% purrr::compact()
} 

# ==== Use of Function: run_bayes_shap_all_models() ====

# Define your model list
model_list <- list(
  BASE      = bayes_base,
  INTL      = bayes_intl,
  INTLDOM   = bayes_intldom,
  DOMPROF   = bayes_domprof,
  INTLPROF  = bayes_intlprof,
  MATCH     = bayes_match
)

results_adver <- run_bayes_shap_all_models(model_list, adver_all, match_vars)

summary_adver_shap <- purrr::imap_dfr(results_adver, ~ tibble::tibble(
  Model = .y,
  Direction_Match_Rate = .x$summary$Direction_Match_Rate,
  Pearson_Correlation   = .x$summary$Pearson_Correlation,
  Abs_Correlation       = .x$summary$Abs_Value_Correlation
))

print(summary_adver_shap)

```

# 3.0 train & test data legis

```{r split-train-test-legis}

processed_legis <- prepare_data_pipeline(
  legis_all, 
  legis_data, 
  match_vars, 
  cat_vars_to_lump, 
  numeric_vars
)

train_data_legis <- processed_legis$train
test_data_legis <- processed_legis$test
```

## 3.1 Predictive Validity legis

### 3.1.1 bayes model training legis

```{r bayes-model-legis}

# check parameters legis
priors_base_legis_check <- get_prior(formula_base, data = train_data_legis, family = bernoulli())
priors_baseline_legis_check <- get_prior(formula_baseline, data = train_data_legis, family = bernoulli())
priors_raw_legis_check  <- get_prior(formula_raw, data = train_data_legis, family = bernoulli())

# consider that employ_indsutry_pl_raw has only 1 level, delet it

# åŽŸ match_vars ä¸­åŽ»æŽ‰ "employ_industry_pl"
match_vars_filtered <- setdiff(match_vars, "employ_industry_pl")

# é‡æ–°ç”Ÿæˆå¸¦ _raw åŽç¼€çš„å˜é‡å
match_features_legis <- paste0(match_vars_filtered, "_raw")

# æž„é€ å…¬å¼
formula_match_legis <- bf(
  as.formula(paste(
    "label ~", 
    paste(match_features_legis, collapse = " + "), 
    "+ (1|issue) + (1|full_name) + (1|congress)"
  ))
)

priors_match_legis_check <- get_prior(formula_match_legis, data = train_data_legis, family = bernoulli())


# use the prior parameters set before

# model training
bayes_base_legis <- brm(formula_base, data = train_data_legis, family = bernoulli(),
                  prior = priors_base, iter = n_iter, chains = n_chains,
                  seed = 123, control = list(adapt_delta = 0.9))

bayes_baseline_legis <- brm(formula_baseline, data = train_data_legis, family = bernoulli(),
                 prior = priors_mechanism, iter = n_iter, chains = n_chains,
                 seed = 123, control = list(adapt_delta = 0.9))

bayes_raw_legis <- brm(formula_raw, data = train_data_legis, family = bernoulli(),
                 prior = priors_mechanism, iter = n_iter, chains = n_chains,
                 seed = 123, control = list(adapt_delta = 0.9))

bayes_match_legis <- brm(formula_match_legis, data = train_data_legis, family = bernoulli(),
                   prior = priors_mechanism, iter = n_iter, chains = n_chains,
                   seed = 123, control = list(adapt_delta = 0.9))

# 1. loo
loo_base_legis <- loo(bayes_base_legis)
loo_baseline_legis <- loo(bayes_baseline_legis)
loo_raw_legis <- loo(bayes_raw_legis)
loo_match_legis <- loo(bayes_match_legis)


# 2. bayes r2 

results <- tibble(
  Model = c("Base-legis", "Baseline-legis", "Raw-legis", "Match-legis"),
  LOOIC = c(loo_base_legis$estimates["looic", "Estimate"],
            loo_baseline_legis$estimates["looic", "Estimate"],
            loo_raw_legis$estimates["looic", "Estimate"],
            loo_match_legis$estimates["looic", "Estimate"]),
  Bayes_R2 = c(bayes_r2_brms(bayes_base_legis),
               bayes_r2_brms(bayes_baseline_legis),
               bayes_r2_brms(bayes_raw_legis),
               bayes_r2_brms(bayes_match_legis))
)

results %>% kable(caption = "Bayesian Model (legis) Comparison: LOOIC and Approximate Bayes-R2")
```

### 3.1.2 posterior diagnosis legis

```{r posterior-diagnosis-legis}

# ==== Use of Function: bayes_posterior_diagnosis() ====

results_base_legis <- bayes_posterior_diagnosis(
  model = bayes_base_legis,
  model_name = "BASE",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_base_diagnosis"
)

results_baseline_legis <- bayes_posterior_diagnosis(
  model = bayes_baseline_legis,
  model_name = "BASELINE",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_base_diagnosis"
)

results_raw_legis <- bayes_posterior_diagnosis(
  model = bayes_raw_legis,
  model_name = "RAW",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_raw_diagnosis"
)

results_match_legis <- bayes_posterior_diagnosis(
  model = bayes_match_legis,
  model_name = "MATCH",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_match_diagnosis"
)
```

### 3.1.3 auc comparison

```{r auc-comparison-legis}

# ==== Use of Function: compare_bayes_auc() ====

# models list
models_legis_compare <- list(
  Base = bayes_base_legis,
  Baseline = bayes_baseline_legis,
  Raw = bayes_raw_legis,
  Match = bayes_match_legis
)

# auc comparsion and tables outprint
compare_bayes_auc(models_legis_compare, test_data = test_data_legis)
```

## 3.2. discriminant validility adver

```{r discriminant-bayes-models-legis}

bayes_base_legis <- brm(formula_base, data = train_data_legis, family = bernoulli(),
                        prior = prior_base, iter = 1000, chains = 2, seed = 123)

bayes_intl_legis <- brm(formula_intl, data = train_data_legis, family = bernoulli(),
                        prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_intldom_legis <- brm(formula_intldom, data = train_data_legis, family = bernoulli(),
                           prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_domprof_legis <- brm(formula_domprof, data = train_data_legis, family = bernoulli(),
                           prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_intlprof_legis <- brm(formula_intlprof, data = train_data_legis, family = bernoulli(),
                            prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_match_legis <- brm(formula_match, data = train_data_legis, family = bernoulli(),
                         prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

# 4.  Bayesian RÂ²

# ==== Use of Function: bayes_r2_brms() ====

r2_mechanism_models_legis <- tibble::tibble(
  Model = c("BASE-legis", "INTL-legis", "INTLDOM-legis", "DOMPROF-legis", "INTLPROF-legis", "MATCH-legis"),
  Pseudo_R2 = c(bayes_r2_brms(bayes_base_legis),
                bayes_r2_brms(bayes_intl_legis),
                bayes_r2_brms(bayes_intldom_legis),
                bayes_r2_brms(bayes_domprof_legis),
                bayes_r2_brms(bayes_intlprof_legis),
                bayes_r2_brms(bayes_match_legis))
)

# 5. tables outprint
knitr::kable(r2_mechanism_models_legis, caption = "Bayesian Pseudo RÂ² Comparison (Mechanism Models)")


# 6. LOO-CV
loo_legis_base     <- loo(bayes_legis_base)
loo_legis_intl     <- loo(bayes_legis_intl)
loo_legis_intldom  <- loo(bayes_legis_intldom)
loo_legis_domprof  <- loo(bayes_legis_domprof)
loo_legis_intlprof <- loo(bayes_legis_intlprof)
loo_legis_match    <- loo(bayes_legis_match)

# elpd_loo
loo_legis_intl$estimates["elpd_loo", "Estimate"]
loo_legis_intldom$estimates["elpd_loo", "Estimate"]
loo_legis_domprof$estimates["elpd_loo", "Estimate"]
loo_legis_intlprof$estimates["elpd_loo", "Estimate"]
loo_legis_match$estimates["elpd_loo", "Estimate"]

# comparison
loo_compare_legis <- loo_compare(
  loo_legis_base, loo_legis_intl, loo_legis_intldom,
  loo_legis_domprof, loo_legis_intlprof, loo_legis_match
)
print(loo_compare_legis)

# visualization
plot(loo_compare_legis, label_points = TRUE, main = "Bayesian Legis Models: LOOIC Comparison")

```

### 3.2.1 variables importance comparions legis
```{r posterior-diagnosis-discriminant-legis, message=FALSE, warning=FALSE}

base_result     <- bayes_value_importance_v2(bayes_base_legis,     model_name = "BASE-legis",     save_dir = save_dir)
intl_result     <- bayes_value_importance_v2(bayes_intl_legis,     model_name = "INTL-legis",     save_dir = save_dir)
intldom_result  <- bayes_value_importance_v2(bayes_intldom_legis,  model_name = "INTLDOM-legis",  save_dir = save_dir)
domprof_result  <- bayes_value_importance_v2(bayes_domprof_legis,  model_name = "DOMPROF-legis",  save_dir = save_dir)
intlprof_result <- bayes_value_importance_v2(bayes_intlprof_legis, model_name = "INTLPROF-legis", save_dir = save_dir)
match_result    <- bayes_value_importance_v2(bayes_match_legis,    model_name = "MATCH-legis",    save_dir = save_dir)

```

### 2.2.2 comparions with shap values legis
```{r shap-direction-bayes-adver, message=FALSE, warning=FALSE}

# ==== Use of Function: run_bayes_shap_all_models() ====

# Define your model list
legis_model_list <- list(
  BASE      = bayes_legis_base,
  INTL      = bayes_legis_intl,
  INTLDOM   = bayes_legis_intldom,
  DOMPROF   = bayes_legis_domprof,
  INTLPROF  = bayes_legis_intlprof,
  MATCH     = bayes_legis_match
)

results_legis <- run_bayes_shap_all_models(legis_model_list, legis_all, match_vars)


summary_legis_shap <- purrr::imap_dfr(results_legis, ~ tibble::tibble(
  Model = .y,
  Direction_Match_Rate = .x$summary$Direction_Match_Rate,
  Pearson_Correlation   = .x$summary$Pearson_Correlation,
  Abs_Correlation       = .x$summary$Abs_Value_Correlation
))

print(summary_legis_shap)
```