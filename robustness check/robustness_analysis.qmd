---
title: "robustness-analysis-bayesian"
author: "Kunya Zhan"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
format: 
  html:
   toc: true
   toc-depth: 3
   number-sections: true
   code-fold: true
   code-summary: "Show/Hide Code"
   smooth-scroll: true
   df-print: paged
   code-tools: true
   highlight-style: atom-one
   toc-location: left
editor: visual
---

# Robustness Analysis

This quarto document aims at reorganizing the codes at 'robustness-check-bayesian'. It serves two main purposes.

1\. Evaluate whether the prediction of legislators' issue-specific advertising and legislative behaviors is reliable (**predictive validity**).

2\. Test whether the identification of the three decision-making modes is consistent and credible (**discriminant validity**)

Accordingly, the variables used in the robustness analysis are aligned with those in the final LightGBM model. However, given the structural characteristics of the dataset, the analysis further controls for the unobserved heterogeneity embedded in legislator, issue, and Congress-session levels. This allows for a cleaner assessment of how the proposed matching mechanism explains legislators' advertising and legislative behaviors on specific issues.

## set up

```{r}
#| label: setup
#| include: false
#| message: false
#| warning: false

# ---- Prevent Mac from sleeping during long render ----
system("caffeinate -dimsu &")
on.exit(system("killall caffeinate"), add = TRUE)
cat("‚òï macOS sleep prevention active for this Quarto render...\n")


# ---- Create a unified output directory ----
output_dir <- "output"
if (!dir.exists(output_dir)) dir.create(output_dir)

# Default chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  fig.path = file.path(output_dir, ""),   # figures from chunks
  cache.path = file.path(output_dir, "cache/")
)

# A helper function to save plots/data consistently
save_to_output <- function(filename, plot = NULL, ...) {
  full_path <- file.path(output_dir, filename)
  
  # Â¶ÇÊûúÊèê‰æõ‰∫Ü plotÔºåÂàôÁî® ggsave ‰øùÂ≠ò
  if (!is.null(plot)) {
    ggplot2::ggsave(filename = full_path, plot = plot, ...)
    message("‚úÖ Saved plot to: ", full_path)
  } else {
    message("‚ö†Ô∏è No plot provided. Returning path only: ", full_path)
  }
  
  return(full_path)
}
```

## Packages Installing if not already installed

```{r}
#| label: install_libraries
#| echo: false
#| message: false
#| warning: false

# ËÆæÁΩÆÈªòËÆ§ CRAN Ê∫êÔºåÈò≤Ê≠¢Êú™ÊåáÂÆöÈïúÂÉèÂØºËá¥ install.packages Êä•Èîô
options(repos = c(CRAN = "https://cloud.r-project.org"))

# ÊâÄÈúÄÂåÖÂàóË°®
packages <- c(
  "readxl", "readr", "dplyr", "tidyr", "lme4", "MuMIn", "pROC", 
  "knitr", "broom", "kableExtra", "alpaca", "marginaleffects", 
  "brms", "tibble", "forcats", "tidyverse", "bayesplot", "ggplot2", 
  "loo", "posterior", "cowplot", "patchwork", "glue", "ggrepel"
)

# Install only if missing
for (pkg in packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
  }
}

# Load all packages
lapply(packages, library, character.only = TRUE)
```

# Data cleansing

## data acquisition

```{r}
#| label: load_data
#| echo: false
#| message: false
#| warning: false

# ----- main databases -----
adver_path <- "/Users/kunyazhan/Documents/GitHub/decodeCompetition/descriptive analysis/input/1022data/adver_analysis_all.xlsx"
legis_path <- "/Users/kunyazhan/Documents/GitHub/decodeCompetition/descriptive analysis/input/1022data/legis_analysis_all.xlsx"

adver_all <- read_excel(adver_path)
legis_all <- read_excel(legis_path)

adver_data <- read_csv("/Users/kunyazhan/Documents/GitHub/decodeCompetition/descriptive analysis/input/1022data/adver_train_data.csv")
legis_data <- read_csv("/Users/kunyazhan/Documents/GitHub/decodeCompetition/descriptive analysis/input/1022data/legis_train_data.csv")
```

## constructing train and test data

```{r}
#| label: data prep pipeline1
#| echo: false
#| message: false
#| warning: false

lump_low_freq_levels <- function(train_df, test_df, vars, prop = 0.05, other_level = "Other") {
  for (v in vars) {
    if (v %in% names(train_df) && v %in% names(test_df)) {
      # Handle NAs explicitly and lump infrequent categories
      train_df[[v]] <- fct_explicit_na(train_df[[v]], na_level = other_level)
      train_df[[v]] <- fct_lump(train_df[[v]], prop = prop, other_level = other_level)
      train_df[[v]] <- droplevels(train_df[[v]])

      # Apply same factor levels to test set
      allowed_levels <- levels(train_df[[v]])
      test_vec <- as.character(fct_explicit_na(test_df[[v]], na_level = other_level))
      test_vec[!test_vec %in% allowed_levels] <- other_level
      test_df[[v]] <- factor(test_vec, levels = allowed_levels)
    }
  }
  list(train = train_df, test = test_df)
}
```

```{r}
#| label: data prep pipeline2
#| echo: false
#| message: false
#| warning: false

align_factor_levels <- function(train, test) {
  factor_vars <- names(train)[sapply(train, is.factor)]
  for (v in factor_vars) {
    if (v %in% names(test)) {
      test[[v]] <- factor(test[[v]], levels = levels(train[[v]]))
    }
  }
  return(test)
}
```

```{r}
#| label: data prep main pipeline
#| echo: false
#| message: false
#| warning: false

prepare_data_pipeline <- function(adver_all, adver_data, match_vars, cat_vars_to_lump, numeric_vars) {
  set.seed(123)

  # --- Step 1: Rename matching features before merging ---
  adver_data_raw <- adver_data %>%
  rename_with(.cols = intersect(names(.), match_vars), .fn = ~ paste0(.x, "_raw"))

  # check whether raw vars were successfully created
  raw_vars <- grep("_raw$", names(adver_data_raw), value = TRUE)
if (length(raw_vars) == 0) {
  stop("No '_raw' variables were created. Please check match_vars and adver_data column names.")
}
  
  # merge into adver_all
  adver_all <- adver_all %>%
  left_join(
    adver_data_raw %>%
      select(full_name, issue, congress, all_of(raw_vars)),
    by = c("full_name", "issue", "congress")
  )

  # --- Step 1.5: Merge related category variables to reduce collinearity ---
  adver_all <- adver_all %>%
    mutate(
      industry_main = coalesce(industry_tp1, industry_tp2, industry_tp3),
      industry_main = fct_lump(fct_explicit_na(industry_main, na_level = "NA"), n = 10),
      ethnicity_tp_main = coalesce(ethnicity_tp1, ethnicity_tp2, ethnicity_tp3),
      ethnicity_tp_main = fct_lump(fct_explicit_na(ethnicity_tp_main, na_level = "NA"), n = 8),
      employ_main = coalesce(employ1, employ2, employ3),
      payroll_main = coalesce(payroll1, payroll2, payroll3),
      ethnicity_main = coalesce(ethnicity1, ethnicity2, ethnicity3)
    )

  # --- Step 2: Split the dataset into training and test subsets ---
  adver_all$pair_id <- paste(adver_all$full_name, adver_all$issue, sep = "_")
  unique_pairs <- unique(adver_all$pair_id)
  train_units <- sample(unique_pairs, size = floor(0.7 * length(unique_pairs)))

  adver_all <- adver_all %>%
    mutate(set = ifelse(pair_id %in% train_units, "train", "test"))

  train_data <- adver_all %>% filter(set == "train")
  test_data <- adver_all %>%
    filter(set == "test" & full_name %in% train_data$full_name & issue %in% train_data$issue)

  # --- Step 3: Convert character variables into factors ---
  vars_to_factor <- c(names(train_data)[sapply(train_data, is.character)], "leverage", "ppvi") %>% unique()
  for (v in vars_to_factor) {
    if (v %in% names(train_data)) {
      train_data[[v]] <- factor(train_data[[v]]) |> droplevels()
    }
    if (v %in% names(test_data)) {
      test_data[[v]] <- factor(test_data[[v]], levels = levels(train_data[[v]]))
    }
  }

  # --- Step 4: Apply low-frequency lumping and explicit NA encoding ---
  lumped <- lump_low_freq_levels(train_data, test_data, cat_vars_to_lump)
  train_data <- lumped$train
  test_data <- lumped$test

  # --- Step 5: Align all factor levels between train/test ---
  test_data <- align_factor_levels(train_data, test_data)

  # --- Step 6: Median imputation and z-score normalization for numeric vars ---
  for (v in numeric_vars) {
    if (v %in% names(train_data)) {
      med <- median(train_data[[v]], na.rm = TRUE)
      train_data[[v]][is.na(train_data[[v]])] <- med
      test_data[[v]][is.na(test_data[[v]])] <- med

      mean_v <- mean(train_data[[v]], na.rm = TRUE)
      sd_v <- sd(train_data[[v]], na.rm = TRUE)

      train_data[[v]] <- (train_data[[v]] - mean_v) / sd_v
      test_data[[v]] <- (test_data[[v]] - mean_v) / sd_v
    }
  }

  # --- Step 6.5: Convert '_raw' variables into factors ---
  match_features <- paste0(match_vars, "_raw")
  cat_match_vars <- setdiff(match_features, c("party_score_pl_raw", "party_score_ne_raw"))
  for (v in cat_match_vars) {
    if (v %in% names(train_data)) {
      train_data[[v]] <- factor(train_data[[v]]) |> droplevels()
    }
    if (v %in% names(test_data)) {
      test_data[[v]] <- factor(test_data[[v]], levels = levels(train_data[[v]]))
    }
  }

  # --- Step 7: Ensure consistent data types across variables ---
  all_vars <- unique(c(
    match_features, "leverage", "s_committee_tp1", "h_committee_tp1", "party_tp_type", 
    "party_tp_score", "industry_main", "ethnicity_main", "employ_main", "payroll_main",
    "ppvi", "npvi", "chamber", "gender", "party", "committee_el",
    "issue", "full_name", "congress"
  ))

  for (v in all_vars) {
    if (v %in% names(train_data) && v %in% names(test_data)) {
      train_class <- class(train_data[[v]])
      if ("factor" %in% train_class) {
        test_data[[v]] <- factor(test_data[[v]], levels = levels(train_data[[v]]))
      } else if ("character" %in% train_class) {
        train_data[[v]] <- factor(train_data[[v]]) |> droplevels()
        test_data[[v]] <- factor(test_data[[v]], levels = levels(train_data[[v]]))
      } else if ("numeric" %in% train_class || "integer" %in% train_class) {
        test_data[[v]] <- as.numeric(test_data[[v]])
      }
    }
  }

  list(train = train_data, test = test_data)
}
```

```{r}
#| label: execute pipeline
#| echo: false
#| message: false
#| warning: false

# ----- prepare the variables categories -----
match_vars <- c(
  "leverage_pl", "leverage_ne", "committee_pl", "committee_ne",
  "employ_industry_pl", "employ_industry_ne", "payroll_industry_pl", "payroll_industry_ne",
  "ethnicity_pl", "ethnicity_ne", "party_value_pl", "party_value_ne",
  "party_score_pl", "party_score_ne"
)

cat_vars_to_lump <- c(
  "h_committee_tp1", "s_committee_tp1", "industry_tp1", "industry_tp2", "industry_tp3", 
  "ethnicity_tp1", "ethnicity_tp2", "ethnicity_tp3", "employ1", "employ2", "employ3", 
  "payroll1", "payroll2", "payroll3", "ethnicity1", "ethnicity2", "ethnicity3"
)

numeric_vars <- c("party_tp_score", "npvi")

# ----- excute -----
# adver
processed_adver <- prepare_data_pipeline(
  adver_all, 
  adver_data, 
  match_vars, 
  cat_vars_to_lump, 
  numeric_vars)

train_data_adver <- processed_adver$train
test_data_adver  <- processed_adver$test

# legis
processed_legis <- prepare_data_pipeline(
  legis_all, 
  legis_data, 
  match_vars, 
  cat_vars_to_lump, 
  numeric_vars
)

train_data_legis <- processed_legis$train
test_data_legis <- processed_legis$test

```

# Predictive Validity

We compared three Bayesian models with progressively enriched feature sets.

The **baseline model** includes only legislators‚Äô demographic and institutional characteristics (first-type features).

The **raw model** adds the original attributes of both legislators and issues (second-type features).

The **match model** incorporates the hypothesized mechanism-based matching features that capture the alignment between legislators and issues (third-type features).

**Predictive validity** is evaluated by comparing the predictive performance of these three models. It is quantified using pseudo-R¬≤ based on posterior means, which reflects the extent to which the inclusion of different feature types improves model fit.

## model preparations

```{r}
#| label: predictive model construction
#| echo: false
#| message: true
#| warning: true

# ----- base model -----
formula_base <- bf(label ~ 1 + (1|issue) + (1|full_name) + (1|congress))

# ----- baseline model -----
# baseline model 
formula_baseline <- bf(label ~ chamber + age + gender + party + seniority +
                         ideology + (1|issue) + (1|full_name) + (1|congress))

# ----- raw model -----

formula_raw <- bf(label ~ leverage + s_committee_tp1 + h_committee_tp1 
                  + party_tp_type + party_tp_score + industry_main 
                  + ethnicity_tp_main + employ_main + payroll_main 
                  + ethnicity_main + ppvi + npvi 
                  + (1|issue) + (1|full_name) + (1|congress))

# ----- match model -----

match_features <- paste0(match_vars, "_raw")
formula_match <- bf(
  as.formula(paste("label ~", paste(match_features, collapse = " + "), "+ (1|issue) + (1|full_name) + (1|congress)"))
)
```

```{r}
#| label: predictive check parameter
#| echo: false
#| message: true
#| warning: true

# adver
priors_base_check <- get_prior(formula_base, data = train_data_adver, family = bernoulli())
priors_baseline_check <- get_prior(formula_baseline, data = train_data_adver, family = bernoulli())
priors_raw_check  <- get_prior(formula_raw, data = train_data_adver, family = bernoulli())
priors_match_check <- get_prior(formula_match, data = train_data_adver, family = bernoulli())

# check parameters legis
priors_base_legis_check <- get_prior(formula_base, data = train_data_legis, family = bernoulli())
priors_baseline_legis_check <- get_prior(formula_baseline, data = train_data_legis, family = bernoulli())
priors_raw_legis_check  <- get_prior(formula_raw, data = train_data_legis, family = bernoulli())
```

```{r}
#| label: predictive priors and iteration
#| echo: false
#| message: true
#| warning: true

# - normal prior (mean=0, sd=2.5) for all fixed effect regression coefficients (class = "b")
# reflecting the expection that most coefficients are centered around zero with moderate variability.
# - heavy-tailed student-t prior (df=3, mean=0, scale=2.5) for the intercept (class = "Intercept")
# and group-levels standard deviations (class = "sd")
# which improves model robustness to outliers and enhances interpretability by allowing more flexibility
# in the presence of extreme values

priors_base <- c(
  set_prior("student_t(3, 0, 2.5)", class = "Intercept"),
  set_prior("student_t(3, 0, 2.5)", class = "sd")
)

priors_mechanism <- c(
  set_prior("normal(0, 2.5)", class = "b"),         
  set_prior("student_t(3, 0, 2.5)", class = "Intercept"),
  set_prior("student_t(3, 0, 2.5)", class = "sd")
)

# iteration parameters setting
# n_iter = 1000 balances between sufficient exploration of the parameter space
# and the computational efficienct
# n_chains = 2 helps assess convergence and ensures robustness of the results,
# while keeping computational cost manageable.

n_iter <- 1000
n_chains <- 2

```

```{r}
#| label: posterior diagnosis function
#| echo: false
#| message: true
#| warning: true

# ==== Function: bayes_posterior_diagnosis() ====
# this section defines a function to summarize bayesian models diagnostics and visualize posterior results:
# - 1. checks convergence diagnostics (rhat, ess)
# - 2. plots posterior distributions and trace plots
# - 3. visualizes fixed effects estimates with 95% ci
# - 4. optionally saves plots to file

bayes_posterior_diagnosis <- function(model, 
                                                 model_name = "MODEL",
                                                 n_trace = 8,
                                                 loo_list = NULL,
                                                 save_path_prefix = NULL) {

  rhat_vals <- rhat(model)
  ess_vals <- neff_ratio(model)

  cat(glue("------ {model_name} ------\n"))
  cat("Rhat > 1.01:\n")
  print(rhat_vals[rhat_vals > 1.01])

  cat("\nESS < 0.1:\n")
  print(ess_vals[ess_vals < 0.1])

  cat("\nSummary of Rhat:\n")
  print(summary(rhat_vals))
  
  cat("\nSummary of ESS ratio:\n")
  print(summary(ess_vals))

posterior_plot <- mcmc_areas(
    as_draws_df(model),
    regex_pars = "^b_",
    prob = 0.8, prob_outer = 0.95
  ) +
    labs(title = glue("Posterior Distributions with 95% CI ({model_name})"))
  
draw_vars <- variables(as_draws_df(model))
trace_candidates <- draw_vars[grepl("^b_", draw_vars)]

if (length(trace_candidates) == 0) {
  warning(glue("No traceable fixed effect parameters for model {model_name}"))
  trace_plot <- ggplot() + labs(title = glue("Trace Plot - {model_name} (No traceable parameters)"))
} else {
  pars_to_plot <- head(trace_candidates, n_trace)
  trace_plot <- mcmc_trace(as_draws_df(model), pars = pars_to_plot) +
    ggtitle(glue("Trace Plot - {model_name} (First {length(pars_to_plot)} parameters)"))
}

  fixef_df <- fixef(model) %>%
    as.data.frame() %>%
    rownames_to_column("Variable") %>%
    arrange(desc(abs(Estimate)))

  fixef_plot <- ggplot(fixef_df, aes(x = reorder(Variable, Estimate), y = Estimate)) +
    geom_point() +
    geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
    coord_flip() +
    labs(title = glue("Fixed Effects Estimates with 95% CI ({model_name})"),
         y = "Estimate", x = "") +
    theme_minimal()

if (!is.null(save_path_prefix)) {
  dir_path <- dirname(save_path_prefix)
  if (!dir.exists(dir_path)) {
    dir.create(dir_path, recursive = TRUE)
  }

  ggsave(paste0(save_path_prefix, "_posterior.png"), posterior_plot, width = 6, height = 4)
  ggsave(paste0(save_path_prefix, "_trace.png"), trace_plot, width = 8, height = 4)
  ggsave(paste0(save_path_prefix, "_fixef.png"), fixef_plot, width = 6, height = 4)
}

  return(list(
    posterior_plot = posterior_plot,
    trace_plot = trace_plot,
    fixef_plot = fixef_plot
  ))
}
```

```{r}
#| label: auc comparsion function
#| echo: false
#| message: true
#| warning: true

# ==== Function: compare_bayes_auc() ====
# this section defines a function to compare auc performance of multiple bayesian logistic models:
# including:
# - 1. computes predicted propabilities on test data
# - 2. plots roc curves for visual comparison
# - 3. performs pairwaise bootsrap tests of auc differences
# - 4. outputs a summary table of auc values


compare_bayes_auc <- function(models_named_list, 
                              test_data, 
                              label_var = "label", 
                              caption = "AUC Comparison on Test Set",
                              colors = NULL,
                              save_path = NULL) {

  model_names <- names(models_named_list)
  if (is.null(colors)) {
    colors <- RColorBrewer::brewer.pal(min(length(models_named_list), 8), "Set1")
  }

  # 1. predicted probabilities & roc
  roc_list <- list()
  auc_list <- c()

  for (i in seq_along(models_named_list)) {
    model <- models_named_list[[i]]
    name  <- model_names[i]
    
    # posterior mean of expected probabilities
    pred_probs <- posterior_epred(model, newdata = test_data, allow_new_levels = TRUE) %>% colMeans()
    
    test_data[[paste0("pred_", name)]] <- pred_probs
    
    # ROC
    roc_obj <- roc(test_data[[label_var]], pred_probs)
    roc_list[[name]] <- roc_obj
    auc_list[i] <- auc(roc_obj)
  }

  # 2. pic
  plot(roc_list[[1]], col = colors[1], lwd = 2, main = "Bayesian Logistic Models: ROC Comparison")
  for (i in 2:length(roc_list)) {
    lines(roc_list[[i]], col = colors[i], lwd = 2)
  }
  legend("bottomright", legend = model_names, col = colors[1:length(roc_list)], lty = 1, lwd = 2)

  if (!is.null(save_path)) {
    dev.copy(png, filename = save_path, width = 800, height = 600)
    dev.off()
  }

# 3. pairwise comparisons
if (length(roc_list) >= 2) {
  cat("\n--- AUC Bootstrap Tests ---\n")
  for (i in 1:(length(roc_list) - 1)) {
    for (j in (i + 1):length(roc_list)) {
      cat(glue::glue("\n{model_names[i]} vs {model_names[j]}:\n"))
      print(roc.test(roc_list[[i]], roc_list[[j]], method = "bootstrap", paired = TRUE))
    }
  }
}

  # 4. AUC tables
  auc_table <- tibble(
    Model = model_names,
    AUC = auc_list
  )
  
  print(knitr::kable(auc_table, digits = 3, caption = caption))
  
  return(list(
    roc_list = roc_list,
    auc_table = auc_table
  ))
}
```

## Adver

### model training

```{r}
#| label: adver_predictive model training
#| echo: false
#| message: true
#| warning: true

# ------ model training ----- 
bayes_base <- brm(formula_base, data = train_data_adver, family = bernoulli(),
                  prior = priors_base, iter = n_iter, chains = n_chains,
                  seed = 123, control = list(adapt_delta = 0.9))

bayes_baseline <- brm(formula_baseline, data = train_data_adver, family = bernoulli(),
                 prior = priors_mechanism, iter = n_iter, chains = n_chains,
                 seed = 123, control = list(adapt_delta = 0.9))

bayes_raw <- brm(formula_raw, data = train_data_adver, family = bernoulli(),
                 prior = priors_mechanism, iter = n_iter, chains = n_chains,
                 seed = 123, control = list(adapt_delta = 0.9))

bayes_match <- brm(formula_match, data = train_data_adver, family = bernoulli(),
                   prior = priors_mechanism, iter = n_iter, chains = n_chains,
                   seed = 123, control = list(adapt_delta = 0.9))
```

```{r}
#| label: adver_predictive model results
#| echo: false
#| message: true
#| warning: true

# ------ LOO ----- 
loo_base <- loo(bayes_base)
loo_baseline <- loo(bayes_baseline)
loo_raw <- loo(bayes_raw)
loo_match <- loo(bayes_match)

# ------ Bayes R2 ----- 
bayes_r2_brms <- function(model) bayes_R2(model)[1]

results <- tibble(
  Model = c("Base","Baseline", "Raw", "Match"),
  LOOIC = c(loo_base$estimates["looic", "Estimate"],
            loo_baseline$estimates["looic", "Estimate"],
            loo_raw$estimates["looic", "Estimate"],
            loo_match$estimates["looic", "Estimate"]),
  Bayes_R2 = c(bayes_r2_brms(bayes_base),
                bayes_r2_brms(bayes_baseline),
                bayes_r2_brms(bayes_raw),
                bayes_r2_brms(bayes_match))
)

results %>% kable(caption = "Bayesian Model Comparison: LOOIC and Approximate Pseudo-R2")
```

### posterior diagnosis

```{r}
#| label: adver_predictive posterior diagnosis
#| echo: false
#| message: true
#| warning: true

results_base <- bayes_posterior_diagnosis(
  model = bayes_base,
  model_name = "BASE",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_base_diagnosis"
)

results_baseline <- bayes_posterior_diagnosis(
  model = bayes_baseline,
  model_name = "BASELINE",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_baseline_diagnosis"
)

results_raw <- bayes_posterior_diagnosis(
  model = bayes_raw,
  model_name = "RAW",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_raw_diagnosis"
)


results_match <-bayes_posterior_diagnosis(
  model = bayes_match,
  model_name = "MATCH",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_match_diagnosis"
)
```

### auc comparison

```{r}
#| label: adver_predictive auc compasion
#| echo: false 
#| message: true 
#| warning: true

# models list
models_adver_compare <- list(
  Base = bayes_base,
  Baseline = bayes_baseline,
  Raw = bayes_raw,
  Match = bayes_match
)

# auc comparsion and tables outprint
compare_bayes_auc(models_adver_compare, test_data = test_data_adver)
```

## Legis

### model training

```{r}
#| label: legis_predictive model prepartion
#| echo: false 
#| message: true 
#| warning: true


```

```{r}
#| label: legis_predictive model training
#| echo: false 
#| message: true 
#| warning: true

bayes_base_legis <- brm(formula_base, data = train_data_legis, family = bernoulli(),
                  prior = priors_base, iter = n_iter, chains = n_chains,
                  seed = 123, control = list(adapt_delta = 0.9))

bayes_baseline_legis <- brm(formula_baseline, data = train_data_legis, family = bernoulli(),
                 prior = priors_mechanism, iter = n_iter, chains = n_chains,
                 seed = 123, control = list(adapt_delta = 0.9))

bayes_raw_legis <- brm(formula_raw, data = train_data_legis, family = bernoulli(),
                 prior = priors_mechanism, iter = n_iter, chains = n_chains,
                 seed = 123, control = list(adapt_delta = 0.9))

bayes_match_legis <- brm(formula_match, data = train_data_legis, family = bernoulli(),
                   prior = priors_mechanism, iter = n_iter, chains = n_chains,
                   seed = 123, control = list(adapt_delta = 0.9))
```

```{r}
#| label: legis_predictive model results
#| echo: false 
#| message: true 
#| warning: true

# 1. loo
loo_base_legis <- loo(bayes_base_legis)
loo_baseline_legis <- loo(bayes_baseline_legis)
loo_raw_legis <- loo(bayes_raw_legis)
loo_match_legis <- loo(bayes_match_legis)


# 2. bayes r2 
results <- tibble(
  Model = c("Base-legis", "Baseline-legis", "Raw-legis", "Match-legis"),
  LOOIC = c(loo_base_legis$estimates["looic", "Estimate"],
            loo_baseline_legis$estimates["looic", "Estimate"],
            loo_raw_legis$estimates["looic", "Estimate"],
            loo_match_legis$estimates["looic", "Estimate"]),
  Bayes_R2 = c(bayes_r2_brms(bayes_base_legis),
               bayes_r2_brms(bayes_baseline_legis),
               bayes_r2_brms(bayes_raw_legis),
               bayes_r2_brms(bayes_match_legis))
)

results %>% kable(caption = "Bayesian Model (legis) Comparison: LOOIC and Approximate Bayes-R2")
```

### posterior diagnosis

```{r}
#| label: legis_predictive posterior diagnosis
#| echo: false 
#| message: true 
#| warning: true

results_base_legis <- bayes_posterior_diagnosis(
  model = bayes_base_legis,
  model_name = "BASE",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_base_diagnosis"
)

results_baseline_legis <- bayes_posterior_diagnosis(
  model = bayes_baseline_legis,
  model_name = "BASELINE",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_base_diagnosis"
)

results_raw_legis <- bayes_posterior_diagnosis(
  model = bayes_raw_legis,
  model_name = "RAW",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_raw_diagnosis"
)

results_match_legis <- bayes_posterior_diagnosis(
  model = bayes_match_legis,
  model_name = "MATCH",
  n_trace = 6,
  save_path_prefix = "outputs/bayes_match_diagnosis"
)
```

### auc comparsion

```{r}
#| label: legis_predictive auc comparison
#| echo: false 
#| message: true 
#| warning: true

models_legis_compare <- list(
  Base = bayes_base_legis,
  Baseline = bayes_baseline_legis,
  Raw = bayes_raw_legis,
  Match = bayes_match_legis
)

compare_bayes_auc(models_legis_compare, test_data = test_data_legis)
```

# Discriminant Validity

**Discriminant validity** is examined by assessing whether the incremental introduction of first-, second-, and third-type features yields consistent and significant improvements in model performance. This is quantified using expected log predictive density (ELPD) under WAIC or LOO-CV criteria.

## model setting

```{r}
#| label: discriminant model constuction
#| echo: false 
#| message: true 
#| warning: true

formula_base <- bf(label ~ 1 + (1|issue) + (1|full_name) + (1|congress))


formula_intl <- bf(label ~ leverage_pl_raw + leverage_ne_raw + (1|issue) + (1|full_name) + (1|congress))


formula_intldom <- bf(label ~ leverage_pl_raw + leverage_ne_raw + ethnicity_pl_raw + ethnicity_ne_raw +
                    payroll_industry_pl_raw + payroll_industry_ne_raw +
                    party_value_pl_raw + party_value_ne_raw +
                    party_score_pl_raw + party_score_ne_raw +
                    employ_industry_pl_raw + employ_industry_ne_raw + 
                    (1|issue) + (1|full_name) + (1|congress))

formula_domprof <- bf(label ~ committee_pl_raw + committee_ne_raw +  ethnicity_pl_raw + ethnicity_ne_raw +
                    payroll_industry_pl_raw + payroll_industry_ne_raw +
                    party_value_pl_raw + party_value_ne_raw +
                    party_score_pl_raw + party_score_ne_raw +
                    employ_industry_pl_raw + employ_industry_ne_raw + 
                    (1|issue) + (1|full_name) + (1|congress))

formula_intlprof <- bf(label ~ leverage_pl_raw + leverage_ne_raw + committee_pl_raw + committee_ne_raw + 
                    (1|issue) + (1|full_name) + (1|congress))

formula_match <- bf(
  as.formula(paste("label ~", paste(match_features, collapse = " + "), "+ (1|issue) + (1|full_name) + (1|congress)"))
)
```

```{r}
#| label: discriminant prior setting 
#| echo: false 
#| message: true 
#| warning: true

# keep prior_base
prior_base <- c(
  set_prior("student_t(3, 0, 2.5)", class = "Intercept"),
  set_prior("student_t(3, 0, 2.5)", class = "sd")
)

# change prior_raw/match into prior_mechanism
prior_mechanism <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 2.5)", class = "Intercept"),
  set_prior("student_t(3, 0, 2.5)", class = "sd")
)
```

```{r}
#| label: variables importance comparison function
#| echo: false 
#| message: true 
#| warning: true

# ==== Function: bayes_value_importance_v2() ====
# this section defines a function visualize and export posterior fixed effect estimates from bayes model
# - 1. ranks predictions by absolute logit coefficient
# - 2. plots estimates with 95% ci
# - 3. optionally saves the plot and the table to disk

bayes_value_importance_v2 <- function(model, model_name = "Model", 
                                      save_dir = file.path(output_dir, "bayes_value_importance"), 
                                      save_plot = TRUE, save_table = TRUE) {
  if (!dir.exists(save_dir)) dir.create(save_dir, recursive = TRUE)

  effects <- as.data.frame(fixef(model)) %>%
    tibble::rownames_to_column("Variable") %>%
    dplyr::filter(!(Variable %in% c("(Intercept)", "Intercept"))) %>%
    dplyr::arrange(desc(abs(Estimate)))

  p <- ggplot(effects, aes(x = reorder(Variable, abs(Estimate)), y = Estimate)) +
    geom_col(fill = "skyblue") +
    geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
    coord_flip() +
    labs(
      title = paste("Posterior Estimates (with 95% CI) -", model_name),
      x = NULL, y = "Logit Coefficient"
    ) +
    theme_minimal(base_size = 13)

  if (save_plot) {
    plot_path <- file.path(save_dir, paste0("bayes_coef_plot_", model_name, ".png"))
    ggsave(plot_path, p, width = 7, height = 5)
    message(glue::glue("‚úÖ plot saves to: {plot_path}"))
  }

  if (save_table) {
    table_path <- file.path(save_dir, paste0("bayes_coef_table_", model_name, ".csv"))
    readr::write_csv(effects, table_path)
    message(glue::glue("üìÑ tables saves to: {table_path}"))
  }

  return(list(plot = p, data = effects))
}
```

### comparions with shap values function

the following three functions was meant to complete: compares bayesian model coefficients with SHAP values from lgbm models, specifically

1.  aligns variables and compares directionality (signs)
2.  computes pearson correlation and direction match rate
3.  visualizes SHAP vs bayesian coefficients in a scatter plot \# major goal: assessing model interpretability consistenct across frameworks

```{r}
#| label: shap values comparison function debug 
#| echo: false 
#| message: true 
#| warning: true

# debug_once(compare_bayes_vs_shap_data)
# results_adver <- run_bayes_shap_all_models(model_list, adver_all, match_vars)

```

```{r}
#| label: shap values comparison function1 
#| echo: false 
#| message: true 
#| warning: true

# ==== Function 1: Extract & Match Coefficients with SHAP (robust version) ====
compare_bayes_vs_shap_data <- function(
  bayes_model,
  shap_data,
  set_name = "test",
  match_vars = NULL,
  suffix = "_raw"
) {
  # 1. extract coef names from brms model
  coef_bayes <- fixef(bayes_model)[, "Estimate"]
  coef_names <- setdiff(names(coef_bayes), c("(Intercept)", "Intercept"))

  # 2. build matching list of coef names (fuzzy matching to capture dummy expansions)
  if (!is.null(match_vars)) {
    vars_in_model <- character(0)
    for (mv in match_vars) {
      # try patterns: with suffix (e.g. leverage_pl_raw) and without suffix (leverage_pl)
      pat1 <- paste0("^", mv, suffix)   # e.g. ^leverage_pl_raw
      pat2 <- paste0("^", mv)           # e.g. ^leverage_pl
      matched <- coef_names[grepl(paste0(pat1, "|", pat2), coef_names)]
      if (length(matched) > 0) {
        vars_in_model <- c(vars_in_model, matched)
      }
    }
    vars_in_model <- unique(vars_in_model)
  } else {
    vars_in_model <- coef_names
  }

  if (length(vars_in_model) == 0) {
    stop("No matched variables found in Bayesian model. Please check match_vars and suffix.")
  }

  # 3. construct table mapping each matched coef name back to its base variable (var_clean)
  #    e.g. leverage_pl_raw1 -> leverage_pl
  map_var_to_base <- function(coef_name, match_vars, suffix) {
    # remove trailing numeric indexes and optional suffix to recover base name
    base <- coef_name
    base <- stringr::str_remove(base, paste0(suffix, "[0-9]*$"))   # drop _raw1/_raw2/_raw etc
    base <- stringr::str_remove(base, "[0-9]+$")                  # drop trailing digits (safety)
    # if still not matching, try to find the match_var that is prefix
    hit <- match_vars[vapply(match_vars, function(mv) grepl(paste0("^", mv), base), logical(1))]
    if (length(hit) >= 1) {
      return(hit[1])
    } else {
      # fallback: drop numeric suffix after last underscore
      return(stringr::str_remove(base, "_[0-9]+$"))
    }
  }

  var_clean_vec <- vapply(vars_in_model, map_var_to_base, FUN.VALUE = character(1), match_vars = match_vars, suffix = suffix)

  coef_bayes_tbl <- tibble::tibble(
    var = vars_in_model,
    coef_logit = as.numeric(coef_bayes[vars_in_model]),
    var_clean = var_clean_vec
  )

  # 4. aggregate coefficients per var_clean (Âπ≥ÂùáÂåñÂêÑ‰∏™ dummy ÁöÑÁ≥ªÊï∞)
  coef_bayes_agg <- coef_bayes_tbl %>%
    dplyr::group_by(var_clean) %>%
    dplyr::summarise(
      coef_logit = mean(coef_logit, na.rm = TRUE),
      n_coefs = dplyr::n(),
      .groups = "drop"
    )

  # 5. ensure shap_data has 'set' column
  if (!"set" %in% names(shap_data)) shap_data$set <- set_name

  # 6. prepare shap variables list (should correspond to var_clean)
  vars_shap <- unique(coef_bayes_agg$var_clean)
  missing_vars <- setdiff(vars_shap, colnames(shap_data))
  if (length(missing_vars) > 0) {
    warning(glue::glue("‚ö†Ô∏è Missing vars in shap_data and will be skipped: {paste(missing_vars, collapse = ', ')}"))
    vars_shap <- setdiff(vars_shap, missing_vars)
    coef_bayes_agg <- coef_bayes_agg %>% filter(var_clean %in% vars_shap)
  }

  if (length(vars_shap) == 0) {
    stop("After checking shap_data, no overlapping variables remain for comparison.")
  }

  # 7. compute mean SHAP per variable (use new across lambda syntax to avoid deprecation warnings)
  shap_values <- shap_data %>%
    dplyr::filter(.data$set == set_name) %>%
    dplyr::summarise(dplyr::across(all_of(vars_shap), \(x) mean(x, na.rm = TRUE))) %>%
    tidyr::pivot_longer(cols = everything(), names_to = "shap_var", values_to = "shap_value")

  # 8. combine aggregated bayes coefs with shap means
  comparison <- coef_bayes_agg %>%
    dplyr::left_join(shap_values, by = c("var_clean" = "shap_var")) %>%
    dplyr::mutate(
      abs_coef_logit = abs(coef_logit),
      abs_shap_value = abs(shap_value),
      dir_logit = sign(coef_logit),
      dir_shap = dplyr::if_else(shap_value == 0, NA_real_, sign(shap_value)),
      direction_match = (dir_logit == dir_shap),
      direction_mismatch = ifelse(is.na(direction_match), NA, !direction_match)
    )

  # 9. summary statistics (handle possible NA in correlations)
  pearson_cor <- tryCatch(cor(comparison$coef_logit, comparison$shap_value, use = "complete.obs"), error = function(e) NA_real_)
  abs_cor <- tryCatch(cor(comparison$abs_coef_logit, comparison$abs_shap_value, use = "complete.obs"), error = function(e) NA_real_)

  summary_stats <- tibble::tibble(
    Direction_Match_Rate = mean(comparison$direction_match, na.rm = TRUE),
    Pearson_Correlation = pearson_cor,
    Abs_Value_Correlation = abs_cor
  )

  return(list(summary = summary_stats, detail = comparison))
}
```

```{r}
#| label: shap values comparison function2 
#| echo: false 
#| message: true 
#| warning: true

# ==== Function 2: Plot SHAP vs Coefficients (adjusted) ====
plot_bayes_vs_shap <- function(comparison_df, model_name = NULL) {
  # ensure expected cols exist
  if (!all(c("coef_logit", "shap_value", "var_clean", "abs_shap_value", "direction_mismatch") %in% colnames(comparison_df))) {
    stop("comparison_df must contain columns: coef_logit, shap_value, var_clean, abs_shap_value, direction_mismatch")
  }

  p <- ggplot2::ggplot(comparison_df, ggplot2::aes(x = coef_logit, y = shap_value, label = var_clean)) +
    ggplot2::geom_point(ggplot2::aes(color = abs_shap_value, shape = direction_mismatch), size = 3) +
    ggplot2::geom_text(vjust = -0.8, size = 3) +
    ggplot2::scale_color_gradient(low = "orange", high = "red", name = "Mean |SHAP|") +
    ggplot2::scale_shape_manual(values = c(`FALSE` = 16, `TRUE` = 17), labels = c("Match", "Mismatch"), name = "Direction Match") +
    ggplot2::geom_vline(xintercept = 0, linetype = "dashed", color = "gray60") +
    ggplot2::geom_hline(yintercept = 0, linetype = "dashed", color = "gray60") +
    ggplot2::labs(
      x = "Posterior Mean (Bayesian logit)",
      y = "Average SHAP Value (LGBM)",
      title = paste("SHAP vs Bayesian Coefficient Directions", ifelse(is.null(model_name), "", paste0(" - ", model_name)))
    ) +
    ggplot2::theme_minimal()

  return(p)
}
```

```{r}
#| label: shap values comparison function3
#| echo: false 
#| message: true 
#| warning: true

# ==== Function 3: Run batch comparisons safely (unchanged interface) ====
run_bayes_shap_all_models <- function(model_list, shap_data, match_vars, set_name = "test", suffix = "_raw") {
  purrr::imap(model_list, ~{
    tryCatch({
      result <- compare_bayes_vs_shap_data(
        bayes_model = .x,
        shap_data   = shap_data,
        set_name    = set_name,
        match_vars  = match_vars,
        suffix      = suffix
      )
      result$plot <- plot_bayes_vs_shap(result$detail, model_name = .y)
      result
    }, error = function(e) {
      message(glue::glue("‚ùå Model {.y} error: {e$message}"))
      return(NULL)
    })
  }) %>% purrr::compact()
}
```

## Adver

### model training

```{r}
#| label: adver_discriminant model training
#| echo: false 
#| message: true 
#| warning: true

# ----- model training ----
bayes_base <- brm(formula_base, data = train_data_adver, family = bernoulli(),
                  prior = prior_base, iter = 1000, chains = 2, seed = 123)

bayes_intl <- brm(formula_intl, data = train_data_adver, family = bernoulli(),
                  prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_intldom <- brm(formula_intldom, data = train_data_adver, family = bernoulli(),
                 prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_domprof <- brm(formula_domprof, data = train_data_adver, family = bernoulli(),
                  prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_intlprof <- brm(formula_intlprof, data = train_data_adver, family = bernoulli(),
                  prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_match <- brm(formula_match, data = train_data_adver, family = bernoulli(),
                  prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)
```

```{r}
#| label: adver_discriminant results
#| echo: false 
#| message: true 
#| warning: true

# ----- loo comparsion -----
loo_base <- loo(bayes_base)
loo_intl <- loo(bayes_intl)
loo_intldom  <- loo(bayes_intldom)
loo_domprof <- loo(bayes_domprof)
loo_intlprof <- loo(bayes_intlprof)
loo_match <- loo(bayes_match)

loo_intl$estimates["elpd_loo", "Estimate"]
loo_intldom$estimates["elpd_loo", "Estimate"]
loo_domprof$estimates["elpd_loo", "Estimate"]
loo_intlprof$estimates["elpd_loo", "Estimate"]
loo_match$estimates["elpd_loo", "Estimate"]

# tables compares loo
loo_compare_mechanism <- loo_compare(loo_base, loo_intl, loo_intldom, loo_domprof, loo_intlprof, loo_match)
print(loo_compare_mechanism)

# visualization loo comparison
plot(loo_compare_mechanism, label_points = TRUE, main = "Bayesian Mechanism Models: LOOIC Comparison")

# ----- bayes r2 -----

r2_mechanism_models_bayes <- tibble::tibble(
  Model = c("BASE", "INTL", "INTLDOM", "DOMPROF", "INTLPROF", "ALL"),
  Pseudo_R2 = c(bayes_r2_brms(bayes_base),
                bayes_r2_brms(bayes_intl),
                bayes_r2_brms(bayes_intldom),
                bayes_r2_brms(bayes_domprof),
                bayes_r2_brms(bayes_intlprof),
                bayes_r2_brms(bayes_match))
)

# 5. tables outprint
knitr::kable(r2_mechanism_models_bayes, caption = "Bayesian Pseudo R¬≤ Comparison (Mechanism Models)")
```

### variables importance comparison

```{r}
#| label: adver_discriminant variables importance comparison 
#| echo: false 
#| message: true 
#| warning: true

base_result     <- bayes_value_importance_v2(bayes_base, model_name = "BASE")
intl_result     <- bayes_value_importance_v2(bayes_intl, model_name = "INTL")
intldom_result  <- bayes_value_importance_v2(bayes_intldom, model_name = "INTLDOM")
domprof_result  <- bayes_value_importance_v2(bayes_domprof, model_name = "DOMPROF")
intlprof_result <- bayes_value_importance_v2(bayes_intlprof, model_name = "INTLPROF")
match_result    <- bayes_value_importance_v2(bayes_match, model_name = "MATCH")
```

### comparions with shap values

```{r}
#| label: adver_discriminant with shap values adver
#| echo: false 
#| message: true 
#| warning: true

model_list <- list(
  INTL      = bayes_intl,
  INTLDOM   = bayes_intldom,
  DOMPROF   = bayes_domprof,
  INTLPROF  = bayes_intlprof,
  MATCH     = bayes_match
)
results_adver <- run_bayes_shap_all_models(model_list, adver_all, match_vars, suffix = "")
print(results_adver)

summary_adver_shap <- purrr::imap_dfr(results_adver, ~ tibble::tibble(
  Model = .y,
  Direction_Match_Rate = .x$summary$Direction_Match_Rate,
  Pearson_Correlation   = .x$summary$Pearson_Correlation,
  Abs_Correlation       = .x$summary$Abs_Value_Correlation
))
print(summary_adver_shap)

```

```{r}
#| label: adver_discriminant with shap values adver debug
#| echo: false 
#| message: true 
#| warning: true

# diagnose_models <- function(model_list, match_vars, suffix = "_raw") {
#  out <- list()
#  for (nm in names(model_list)) {
#    mod <- model_list[[nm]]
#    cat("\n---- MODEL:", nm, "----\n")
#    coef_b <- tryCatch(names(fixef(mod)[, "Estimate"]), error = function(e) NULL)
#    print(head(coef_b, 30))
#    if (is.null(coef_b)) {
#      cat("  >> cannot extract fixef() names (model type mismatch?)\n")
#      out[[nm]] <- list(coef_names = NA_character_, matched = character(0))
#      next
#    }
#    # exact intersections
#    inter1 <- intersect(coef_b, match_vars)
#    inter2 <- intersect(coef_b, paste0(match_vars, suffix))
#    # fuzzy: coef names that start with any match_var or match_var+suffix
#    fuzzy_matches <- coef_b[sapply(coef_b, function(x) any(vapply(match_vars, function(mv) #grepl(paste0("^", mv), x), logical(1))))]
#    fuzzy_with_suffix <- coef_b[sapply(coef_b, function(x) any(vapply(paste0(match_vars, #suffix), function(mv) grepl(paste0("^", mv), x), logical(1))))]
#    cat("  exact intersect with match_vars:", toString(inter1), "\n")
#    cat("  exact intersect with match_vars+suffix:", toString(inter2), "\n")
#    cat("  fuzzy matches (startsWith match_vars):", toString(unique(fuzzy_matches)), "\n")
#    cat("  fuzzy matches (startsWith match_vars+suffix):", toString(unique(fuzzy_with_suffix)), "\n")
#    out[[nm]] <- list(coef_names = coef_b, exact = c(inter1, inter2), fuzzy = unique(c(fuzzy_matches, fuzzy_with_suffix)))
#  }
#  return(out)
#}

#diag_res <- diagnose_models(model_list, match_vars, suffix = "")
```

## Legis

### model training

```{r}
#| label: legis_discriminant model training
#| echo: false 
#| message: true 
#| warning: true

bayes_base_legis <- brm(formula_base, data = train_data_legis, family = bernoulli(),
                        prior = prior_base, iter = 1000, chains = 2, seed = 123)

bayes_intl_legis <- brm(formula_intl, data = train_data_legis, family = bernoulli(),
                        prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_intldom_legis <- brm(formula_intldom, data = train_data_legis, family = bernoulli(),
                           prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_domprof_legis <- brm(formula_domprof, data = train_data_legis, family = bernoulli(),
                           prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_intlprof_legis <- brm(formula_intlprof, data = train_data_legis, family = bernoulli(),
                            prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)

bayes_match_legis <- brm(formula_match, data = train_data_legis, family = bernoulli(),
                         prior = prior_mechanism, iter = 1000, chains = 2, seed = 123)
```

```{r}
#| label: legis_discriminant model results
#| echo: false 
#| message: true 
#| warning: true

# ----- LOO-CV -----
loo_legis_base     <- loo(bayes_base_legis)
loo_legis_intl     <- loo(bayes_intl_legis)
loo_legis_intldom  <- loo(bayes_intldom_legis)
loo_legis_domprof  <- loo(bayes_domprof_legis)
loo_legis_intlprof <- loo(bayes_intlprof_legis)
loo_legis_match    <- loo(bayes_match_legis)

# elpd_loo
loo_legis_intl$estimates["elpd_loo", "Estimate"]
loo_legis_intldom$estimates["elpd_loo", "Estimate"]
loo_legis_domprof$estimates["elpd_loo", "Estimate"]
loo_legis_intlprof$estimates["elpd_loo", "Estimate"]
loo_legis_match$estimates["elpd_loo", "Estimate"]

# comparison
loo_compare_legis <- loo_compare(
  loo_legis_base, loo_legis_intl, loo_legis_intldom,
  loo_legis_domprof, loo_legis_intlprof, loo_legis_match
)
print(loo_compare_legis)

# visualization
plot(loo_compare_legis, label_points = TRUE, main = "Bayesian Legis Models: LOOIC Comparison")

# ----- Bayesian R¬≤ -----
r2_mechanism_models_legis <- tibble::tibble(
  Model = c("BASE-legis", "INTL-legis", "INTLDOM-legis", "DOMPROF-legis", "INTLPROF-legis", "MATCH-legis"),
  Pseudo_R2 = c(bayes_r2_brms(bayes_base_legis),
                bayes_r2_brms(bayes_intl_legis),
                bayes_r2_brms(bayes_intldom_legis),
                bayes_r2_brms(bayes_domprof_legis),
                bayes_r2_brms(bayes_intlprof_legis),
                bayes_r2_brms(bayes_match_legis))
)

# tables outprint
knitr::kable(r2_mechanism_models_legis, caption = "Bayesian Pseudo R¬≤ Comparison (Mechanism Models)")
```

### variables importance comparison

```{r}
#| label: legis_discriminant variables importance comparison
#| echo: false 
#| message: true 
#| warning: true

base_result     <- bayes_value_importance_v2(bayes_base_legis, model_name = "BASE-legis")
intl_result     <- bayes_value_importance_v2(bayes_intl_legis, model_name = "INTL-legis")
intldom_result  <- bayes_value_importance_v2(bayes_intldom_legis, model_name = "INTLDOM-legis")
domprof_result  <- bayes_value_importance_v2(bayes_domprof_legis, model_name = "DOMPROF-legis")
intlprof_result <- bayes_value_importance_v2(bayes_intlprof_legis, model_name = "INTLPROF-legis")
match_result    <- bayes_value_importance_v2(bayes_match_legis, model_name = "MATCH-legis")
```

### comparions with shap values

```{r}
#| label: legis_discriminant comparions with shap values
#| echo: false 
#| message: true 
#| warning: true

legis_model_list <- list(
  INTL      = bayes_intl_legis,
  INTLDOM   = bayes_intldom_legis,
  DOMPROF   = bayes_domprof_legis,
  INTLPROF  = bayes_intlprof_legis,
  MATCH     = bayes_match_legis
)
results_legis <- run_bayes_shap_all_models(legis_model_list, legis_all, match_vars)

summary_legis_shap <- purrr::imap_dfr(results_legis, ~ tibble::tibble(
  Model = .y,
  Direction_Match_Rate = .x$summary$Direction_Match_Rate,
  Pearson_Correlation   = .x$summary$Pearson_Correlation,
  Abs_Correlation       = .x$summary$Abs_Value_Correlation
))

print(summary_legis_shap)
```
